{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸº Vesuvius Challenge - SwinUNETR-V2 Real Data Only Training\n",
    "## å®ŸKaggleãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå°‚ç”¨ã®SwinUNETR-V2å­¦ç¿’ã‚³ãƒ¼ãƒ‰\n",
    "\n",
    "### ç‰¹å¾´\n",
    "- âœ… **å®Ÿãƒ‡ãƒ¼ã‚¿å°‚ç”¨**: Kaggle Vesuviusãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã¿ä½¿ç”¨\n",
    "- âœ… **SwinUNETR-V2**: æœ€æ–°ã®Shifted Window Transformer\n",
    "- âœ… **3Då®Œå…¨å¯¾å¿œ**: 3Dãƒœãƒªãƒ¥ãƒ¼ãƒ (96,96,64)å‡¦ç†\n",
    "- âœ… **GPUè‡ªå‹•æœ€é©åŒ–**: A100/A6000/RTXç³»å¯¾å¿œ\n",
    "- âœ… **Mixed Precision**: é«˜é€Ÿå­¦ç¿’\n",
    "- ğŸ¯ **Target**: Kaggle LB 0.552+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç’°å¢ƒè¨­å®šã¨ä¾å­˜é–¢ä¿‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# è­¦å‘Šéè¡¨ç¤º\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "print(\"ğŸº Vesuvius Challenge - SwinUNETR-V2 Real Data Only Training\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"TensorFlow: {tf.__version__}\")\n",
    "print(f\"NumPy: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Kaggle APIè¨­å®šã¨ãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ€ãƒ¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle APIã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆå¿…è¦ãªå ´åˆï¼‰\n",
    "try:\n",
    "    import kaggle\n",
    "    print(\"âœ… Kaggle APIç¢ºèªæ¸ˆã¿\")\n",
    "except ImportError:\n",
    "    print(\"ğŸ“¦ Kaggle APIã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­...\")\n",
    "    !pip install kaggle -q\n",
    "    import kaggle\n",
    "    print(\"âœ… Kaggle APIã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ€ãƒ¼é–¢æ•°å®šç¾©\n",
    "\n",
    "import json\n",
    "import zipfile\n",
    "import shutil\n",
    "from PIL import Image\n",
    "\n",
    "def setup_kaggle_api():\n",
    "    \"\"\"Kaggle APIè¨­å®š\"\"\"\n",
    "    print(\"ğŸ”‘ Kaggle APIè¨­å®šä¸­...\")\n",
    "    \n",
    "    # Kaggleèªè¨¼æƒ…å ±ã®å ´æ‰€\n",
    "    possible_kaggle_paths = [\n",
    "        Path.home() / '.kaggle' / 'kaggle.json',\n",
    "        Path('/root/.kaggle/kaggle.json'),  # Runpods\n",
    "        Path('./kaggle.json'),  # ç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª\n",
    "        Path('/workspace/kaggle.json'),  # Runpods workspace\n",
    "        Path('/kaggle/input/kaggle.json'),  # Kaggle notebook\n",
    "    ]\n",
    "    \n",
    "    kaggle_json = None\n",
    "    for path in possible_kaggle_paths:\n",
    "        if path.exists():\n",
    "            kaggle_json = path\n",
    "            print(f\"âœ… Kaggleèªè¨¼ãƒ•ã‚¡ã‚¤ãƒ«ç™ºè¦‹: {path}\")\n",
    "            break\n",
    "    \n",
    "    if kaggle_json is None:\n",
    "        print(\"âŒ kaggle.json ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n",
    "        print(\"ğŸ“ Kaggleèªè¨¼è¨­å®šæ‰‹é †:\")\n",
    "        print(\"   1. https://www.kaggle.com/settings/account\")\n",
    "        print(\"   2. 'Create New Token'ã§kaggle.jsonãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\")\n",
    "        print(\"   3. ä»¥ä¸‹ã®ã„ãšã‚Œã‹ã«é…ç½®:\")\n",
    "        for path in possible_kaggle_paths:\n",
    "            print(f\"      {path}\")\n",
    "        return False\n",
    "    \n",
    "    # Kaggleãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ\n",
    "    kaggle_dir = Path.home() / '.kaggle'\n",
    "    kaggle_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚³ãƒ”ãƒ¼ï¼ˆå¿…è¦ãªå ´åˆï¼‰\n",
    "    target_kaggle = kaggle_dir / 'kaggle.json'\n",
    "    if not target_kaggle.exists():\n",
    "        shutil.copy2(kaggle_json, target_kaggle)\n",
    "    \n",
    "    # ãƒ‘ãƒ¼ãƒŸãƒƒã‚·ãƒ§ãƒ³è¨­å®š\n",
    "    target_kaggle.chmod(0o600)\n",
    "    \n",
    "    return True\n",
    "\n",
    "def download_vesuvius_dataset(download_dir=\"./vesuvius_data\"):\n",
    "    \"\"\"Vesuvius Challengeãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\"\"\"\n",
    "    print(f\"\\nğŸ“¥ Vesuviusãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...\")\n",
    "    \n",
    "    download_path = Path(download_dir)\n",
    "    download_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Vesuvius Challengeç«¶æŠ€ãƒ‡ãƒ¼ã‚¿\n",
    "    competition = \"vesuvius-challenge-surface-detection\"\n",
    "    \n",
    "    print(f\"ğŸ† ç«¶æŠ€ãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰: {competition}\")\n",
    "    try:\n",
    "        # Kaggle APIçµŒç”±ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
    "        cmd = f\"cd {download_path} && kaggle competitions download -c {competition}\"\n",
    "        result = os.system(cmd)\n",
    "        \n",
    "        if result == 0:\n",
    "            print(f\"âœ… {competition} ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æˆåŠŸ\")\n",
    "            \n",
    "            # ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚’å±•é–‹\n",
    "            zip_files = list(download_path.glob(f\"{competition}.zip\"))\n",
    "            for zip_file in zip_files:\n",
    "                print(f\"ğŸ“¦ å±•é–‹ä¸­: {zip_file}\")\n",
    "                with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "                    extract_dir = download_path / competition.replace('-', '_')\n",
    "                    extract_dir.mkdir(exist_ok=True)\n",
    "                    zip_ref.extractall(extract_dir)\n",
    "                \n",
    "                # ZIPãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ï¼ˆå®¹é‡ç¯€ç´„ï¼‰\n",
    "                zip_file.unlink()\n",
    "                print(f\"âœ… å±•é–‹å®Œäº†: {extract_dir}\")\n",
    "        else:\n",
    "            print(f\"âŒ {competition} ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ã‚¨ãƒ©ãƒ¼: {competition} - {e}\")\n",
    "    \n",
    "    return download_path\n",
    "\n",
    "# APIè¨­å®šãƒ†ã‚¹ãƒˆ\n",
    "api_ready = setup_kaggle_api()\n",
    "print(f\"Kaggle APIæº–å‚™çŠ¶æ³: {'âœ… æº–å‚™å®Œäº†' if api_ready else 'âŒ è¦è¨­å®š'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã‚¯ãƒ©ã‚¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VesuviusRealDataProcessor:\n",
    "    \"\"\"å®ŸVesuviusãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ã‚¯ãƒ©ã‚¹\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir, output_dir=\"./processed_vesuvius\"):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        print(f\"ğŸ“Š ãƒ‡ãƒ¼ã‚¿å‡¦ç†åˆæœŸåŒ–:\")\n",
    "        print(f\"   å…¥åŠ›: {self.data_dir}\")\n",
    "        print(f\"   å‡ºåŠ›: {self.output_dir}\")\n",
    "    \n",
    "    def find_data_files(self):\n",
    "        \"\"\"ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ç´¢\"\"\"\n",
    "        print(\"ğŸ” ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«æ¢ç´¢ä¸­...\")\n",
    "        \n",
    "        # TIFFãƒ•ã‚¡ã‚¤ãƒ«æ¢ç´¢\n",
    "        tiff_files = []\n",
    "        for pattern in [\"*.tif\", \"*.tiff\", \"*.TIF\", \"*.TIFF\"]:\n",
    "            tiff_files.extend(list(self.data_dir.rglob(pattern)))\n",
    "        \n",
    "        # è¨“ç·´ç”»åƒã¨ãƒ©ãƒ™ãƒ«åˆ†é¡\n",
    "        train_images = []\n",
    "        train_labels = []\n",
    "        test_images = []\n",
    "        \n",
    "        for tiff_file in tiff_files:\n",
    "            if \"train\" in str(tiff_file).lower():\n",
    "                if \"label\" in str(tiff_file).lower() or \"mask\" in str(tiff_file).lower():\n",
    "                    train_labels.append(tiff_file)\n",
    "                else:\n",
    "                    train_images.append(tiff_file)\n",
    "            elif \"test\" in str(tiff_file).lower():\n",
    "                test_images.append(tiff_file)\n",
    "        \n",
    "        print(f\"ğŸ“Š ç™ºè¦‹ãƒ•ã‚¡ã‚¤ãƒ«æ•°:\")\n",
    "        print(f\"   è¨“ç·´ç”»åƒ: {len(train_images)}\")\n",
    "        print(f\"   è¨“ç·´ãƒ©ãƒ™ãƒ«: {len(train_labels)}\")\n",
    "        print(f\"   ãƒ†ã‚¹ãƒˆç”»åƒ: {len(test_images)}\")\n",
    "        \n",
    "        return {\n",
    "            'train_images': sorted(train_images),\n",
    "            'train_labels': sorted(train_labels),\n",
    "            'test_images': sorted(test_images)\n",
    "        }\n",
    "    \n",
    "    def create_3d_volumes(self, image_files, label_files=None, volume_size=(96, 96, 64)):\n",
    "        \"\"\"2Dç”»åƒã‹ã‚‰3Dãƒœãƒªãƒ¥ãƒ¼ãƒ ä½œæˆ\"\"\"\n",
    "        print(f\"ğŸ§Š 3Dãƒœãƒªãƒ¥ãƒ¼ãƒ ä½œæˆä¸­... (ç›®æ¨™ã‚µã‚¤ã‚º: {volume_size})\")\n",
    "        \n",
    "        volumes = []\n",
    "        labels = [] if label_files else None\n",
    "        \n",
    "        # ãƒœãƒªãƒ¥ãƒ¼ãƒ ç”Ÿæˆï¼ˆé€£ç¶šã‚¹ãƒ©ã‚¤ã‚¹ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ï¼‰\n",
    "        slices_per_volume = volume_size[2]  # Depth\n",
    "        \n",
    "        for i in range(0, len(image_files), slices_per_volume):\n",
    "            volume_images = image_files[i:i + slices_per_volume]\n",
    "            \n",
    "            if len(volume_images) < slices_per_volume:\n",
    "                print(f\"âš ï¸ ã‚¹ãƒ©ã‚¤ã‚¹ä¸è¶³ã§ã‚¹ã‚­ãƒƒãƒ—: {len(volume_images)}\")\n",
    "                continue\n",
    "            \n",
    "            # ç”»åƒèª­ã¿è¾¼ã¿\n",
    "            volume_slices = []\n",
    "            label_slices = [] if label_files else None\n",
    "            \n",
    "            for j, img_file in enumerate(volume_images):\n",
    "                try:\n",
    "                    # ç”»åƒèª­ã¿è¾¼ã¿\n",
    "                    img = Image.open(img_file)\n",
    "                    img_array = np.array(img, dtype=np.float32)\n",
    "                    \n",
    "                    # ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«å¤‰æ›\n",
    "                    if len(img_array.shape) == 3:\n",
    "                        img_array = np.mean(img_array, axis=2)\n",
    "                    \n",
    "                    # ãƒªã‚µã‚¤ã‚º\n",
    "                    img_resized = tf.image.resize(\n",
    "                        tf.expand_dims(img_array, -1),\n",
    "                        [volume_size[0], volume_size[1]]\n",
    "                    ).numpy().squeeze()\n",
    "                    \n",
    "                    volume_slices.append(img_resized)\n",
    "                    \n",
    "                    # ãƒ©ãƒ™ãƒ«å‡¦ç†\n",
    "                    if label_files and i + j < len(label_files):\n",
    "                        label_file = label_files[i + j]\n",
    "                        if label_file.exists():\n",
    "                            label_img = Image.open(label_file)\n",
    "                            label_array = np.array(label_img, dtype=np.uint8)\n",
    "                            \n",
    "                            if len(label_array.shape) == 3:\n",
    "                                label_array = label_array[:, :, 0]\n",
    "                            \n",
    "                            # ãƒ©ãƒ™ãƒ«ãƒªã‚µã‚¤ã‚º\n",
    "                            label_resized = tf.image.resize(\n",
    "                                tf.expand_dims(label_array, -1),\n",
    "                                [volume_size[0], volume_size[1]],\n",
    "                                method='nearest'\n",
    "                            ).numpy().squeeze()\n",
    "                            \n",
    "                            label_slices.append(label_resized)\n",
    "                        else:\n",
    "                            # ãƒ€ãƒŸãƒ¼ãƒ©ãƒ™ãƒ«\n",
    "                            dummy_label = np.zeros((volume_size[0], volume_size[1]))\n",
    "                            label_slices.append(dummy_label)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"âŒ ç”»åƒèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {img_file} - {e}\")\n",
    "                    continue\n",
    "            \n",
    "            if len(volume_slices) == slices_per_volume:\n",
    "                # 3Dãƒœãƒªãƒ¥ãƒ¼ãƒ ä½œæˆ\n",
    "                volume_3d = np.stack(volume_slices, axis=2)  # (H, W, D)\n",
    "                volume_3d = np.expand_dims(volume_3d, axis=-1)  # (H, W, D, 1)\n",
    "                \n",
    "                # æ­£è¦åŒ–\n",
    "                volume_3d = volume_3d / 255.0\n",
    "                volumes.append(volume_3d)\n",
    "                \n",
    "                if label_files and len(label_slices) == slices_per_volume:\n",
    "                    label_3d = np.stack(label_slices, axis=2)  # (H, W, D)\n",
    "                    \n",
    "                    # ãƒã‚¤ãƒŠãƒªåŒ–\n",
    "                    label_3d = (label_3d > 127).astype(np.uint8)\n",
    "                    \n",
    "                    # One-hot encoding\n",
    "                    label_categorical = np.zeros((*label_3d.shape, 2))\n",
    "                    label_categorical[..., 0] = 1 - label_3d  # background\n",
    "                    label_categorical[..., 1] = label_3d      # surface\n",
    "                    \n",
    "                    labels.append(label_categorical)\n",
    "                \n",
    "                fg_ratio = np.mean(label_3d) if label_files else 0\n",
    "                print(f\"âœ… ãƒœãƒªãƒ¥ãƒ¼ãƒ {len(volumes)}ä½œæˆ - å‰æ™¯æ¯”ç‡: {fg_ratio:.3f}\")\n",
    "        \n",
    "        print(f\"ğŸ¯ ä½œæˆå®Œäº†: {len(volumes)}å€‹ã®ãƒœãƒªãƒ¥ãƒ¼ãƒ \")\n",
    "        \n",
    "        return np.array(volumes), np.array(labels) if labels else None\n",
    "    \n",
    "    def save_processed_data(self, volumes, labels=None, prefix=\"vesuvius\"):\n",
    "        \"\"\"å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜\"\"\"\n",
    "        print(\"ğŸ’¾ å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜ä¸­...\")\n",
    "        \n",
    "        # NumPyå½¢å¼ã§ä¿å­˜\n",
    "        volumes_path = self.output_dir / f\"{prefix}_volumes.npy\"\n",
    "        np.save(volumes_path, volumes)\n",
    "        print(f\"âœ… ãƒœãƒªãƒ¥ãƒ¼ãƒ ä¿å­˜: {volumes_path}\")\n",
    "        \n",
    "        labels_path = None\n",
    "        if labels is not None:\n",
    "            labels_path = self.output_dir / f\"{prefix}_labels.npy\"\n",
    "            np.save(labels_path, labels)\n",
    "            print(f\"âœ… ãƒ©ãƒ™ãƒ«ä¿å­˜: {labels_path}\")\n",
    "        \n",
    "        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜\n",
    "        metadata = {\n",
    "            'num_volumes': len(volumes),\n",
    "            'volume_shape': volumes.shape[1:],\n",
    "            'has_labels': labels is not None,\n",
    "            'label_shape': labels.shape[1:] if labels is not None else None,\n",
    "            'data_type': str(volumes.dtype),\n",
    "            'created_by': 'swinunetr_v2_real_data_only.ipynb'\n",
    "        }\n",
    "        \n",
    "        metadata_path = self.output_dir / f\"{prefix}_metadata.json\"\n",
    "        with open(metadata_path, 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "        \n",
    "        print(f\"âœ… ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜: {metadata_path}\")\n",
    "        print(f\"ğŸ“Š ä¿å­˜ã‚µãƒãƒªãƒ¼: {len(volumes)}ãƒœãƒªãƒ¥ãƒ¼ãƒ , å½¢çŠ¶{volumes.shape}\")\n",
    "        \n",
    "        return volumes_path, labels_path\n",
    "\n",
    "print(\"âœ… ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã‚¯ãƒ©ã‚¹å®šç¾©å®Œäº†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. GPUè¨­å®šã¨ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_gpu_and_config():\n",
    "    \"\"\"GPUè¨­å®šã¨å­¦ç¿’è¨­å®š\"\"\"\n",
    "    \n",
    "    # GPUè¨­å®š\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            \n",
    "            gpu_details = tf.config.experimental.get_device_details(gpus[0])\n",
    "            gpu_name = gpu_details.get('device_name', 'Unknown GPU')\n",
    "            print(f\"âœ… GPUæ¤œå‡º: {gpu_name}\")\n",
    "            \n",
    "            # GPUåˆ¥æœ€é©åŒ–\n",
    "            if 'A100' in gpu_name or 'V100' in gpu_name:\n",
    "                batch_size, mixed_precision = 8, True\n",
    "                print(\"ğŸš€ é«˜æ€§èƒ½GPU - æœ€å¤§è¨­å®š\")\n",
    "            elif 'A6000' in gpu_name or 'RTX 4090' in gpu_name:\n",
    "                batch_size, mixed_precision = 6, True\n",
    "                print(\"ğŸ’ª é«˜æ€§èƒ½GPU - æ¨™æº–è¨­å®š\")\n",
    "            else:\n",
    "                batch_size, mixed_precision = 4, True\n",
    "                print(\"ğŸ“Š æ¨™æº–GPU - ãƒãƒ©ãƒ³ã‚¹è¨­å®š\")\n",
    "                \n",
    "        except RuntimeError as e:\n",
    "            print(f\"âš ï¸ GPUè¨­å®šã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "            batch_size, mixed_precision = 2, False\n",
    "    else:\n",
    "        print(\"âŒ GPUæœªæ¤œå‡º - CPUå®Ÿè¡Œ\")\n",
    "        batch_size, mixed_precision = 1, False\n",
    "    \n",
    "    # Mixed Precision\n",
    "    if mixed_precision:\n",
    "        tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "        print(\"âœ… Mixed Precisionæœ‰åŠ¹åŒ–\")\n",
    "    \n",
    "    # è¨­å®š\n",
    "    config = {\n",
    "        'INPUT_SHAPE': (96, 96, 64, 1),\n",
    "        'NUM_CLASSES': 2,\n",
    "        'BATCH_SIZE': batch_size,\n",
    "        'EPOCHS': 100,\n",
    "        'LEARNING_RATE': 1e-4,\n",
    "        'WEIGHT_DECAY': 1e-5,\n",
    "        \n",
    "        # SwinUNETR-V2è¨­å®š\n",
    "        'DEPTHS': (2, 2, 6, 2),\n",
    "        'NUM_HEADS': (3, 6, 12, 24),\n",
    "        'EMBED_DIM': 96,\n",
    "        'WINDOW_SIZE': (7, 7, 7),\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nğŸ“Š è¨­å®šå®Œäº†:\")\n",
    "    print(f\"   ãƒãƒƒãƒã‚µã‚¤ã‚º: {config['BATCH_SIZE']}\")\n",
    "    print(f\"   å…¥åŠ›å½¢çŠ¶: {config['INPUT_SHAPE']}\")\n",
    "    print(f\"   Mixed Precision: {mixed_precision}\")\n",
    "    \n",
    "    return config\n",
    "\n",
    "# GPUè¨­å®šå®Ÿè¡Œ\n",
    "CONFIG = setup_gpu_and_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. å®Ÿãƒ‡ãƒ¼ã‚¿æº–å‚™ï¼ˆå¿…é ˆï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_real_data(config):\n",
    "    \"\"\"å®Ÿãƒ‡ãƒ¼ã‚¿æº–å‚™ï¼ˆå¿…é ˆï¼‰\"\"\"\n",
    "    print(\"\\nğŸ“Š å®Ÿãƒ‡ãƒ¼ã‚¿æº–å‚™ä¸­...\")\n",
    "    \n",
    "    # æ—¢å­˜ã®å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ãƒã‚§ãƒƒã‚¯\n",
    "    processed_data_paths = [\n",
    "        \"./processed_vesuvius/vesuvius_real_volumes.npy\",\n",
    "        \"./vesuvius_data/processed/vesuvius_real_volumes.npy\",\n",
    "        \"../input/vesuvius-challenge-surface-detection/processed_volumes.npy\",\n",
    "    ]\n",
    "    \n",
    "    volumes_path = None\n",
    "    labels_path = None\n",
    "    \n",
    "    for path in processed_data_paths:\n",
    "        volumes_file = Path(path)\n",
    "        labels_file = Path(str(path).replace('volumes', 'labels'))\n",
    "        \n",
    "        if volumes_file.exists():\n",
    "            volumes_path = volumes_file\n",
    "            labels_path = labels_file if labels_file.exists() else None\n",
    "            print(f\"âœ… å‡¦ç†æ¸ˆã¿å®Ÿãƒ‡ãƒ¼ã‚¿ç™ºè¦‹: {volumes_file}\")\n",
    "            break\n",
    "    \n",
    "    # å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã€Kaggleã‹ã‚‰å–å¾—ãƒ»å‡¦ç†\n",
    "    if not volumes_path:\n",
    "        if not api_ready:\n",
    "            raise Exception(\"âŒ Kaggle APIè¨­å®šã«å¤±æ•—ã€‚kaggle.jsonã‚’é…ç½®ã—ã¦ãã ã•ã„ã€‚\")\n",
    "        \n",
    "        print(\"ğŸ”„ Kaggleã‹ã‚‰å®Ÿãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»å‰å‡¦ç†ä¸­...\")\n",
    "        \n",
    "        # ãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
    "        download_dir = download_vesuvius_dataset(\"./vesuvius_real_data\")\n",
    "        \n",
    "        # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†\n",
    "        processor = VesuviusRealDataProcessor(\n",
    "            data_dir=download_dir,\n",
    "            output_dir=\"./processed_vesuvius\"\n",
    "        )\n",
    "        \n",
    "        data_files = processor.find_data_files()\n",
    "        \n",
    "        if len(data_files['train_images']) == 0:\n",
    "            raise Exception(\"âŒ è¨“ç·´ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚Kaggleãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\")\n",
    "        \n",
    "        print(f\"ğŸ“Š ç™ºè¦‹ã•ã‚ŒãŸç”»åƒãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(data_files['train_images'])}\")\n",
    "        print(f\"ğŸ“Š ç™ºè¦‹ã•ã‚ŒãŸãƒ©ãƒ™ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(data_files['train_labels'])}\")\n",
    "        \n",
    "        # 3Dãƒœãƒªãƒ¥ãƒ¼ãƒ ä½œæˆ\n",
    "        volumes, labels = processor.create_3d_volumes(\n",
    "            data_files['train_images'],\n",
    "            data_files['train_labels'],\n",
    "            volume_size=config['INPUT_SHAPE'][:3]\n",
    "        )\n",
    "        \n",
    "        if len(volumes) == 0:\n",
    "            raise Exception(\"âŒ 3Dãƒœãƒªãƒ¥ãƒ¼ãƒ ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚\")\n",
    "        \n",
    "        # å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜\n",
    "        volumes_path, labels_path = processor.save_processed_data(\n",
    "            volumes, labels, prefix=\"vesuvius_real\"\n",
    "        )\n",
    "        print(\"âœ… å®Ÿãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»å‰å‡¦ç†å®Œäº†\")\n",
    "    \n",
    "    return volumes_path, labels_path\n",
    "\n",
    "# å®Ÿãƒ‡ãƒ¼ã‚¿æº–å‚™å®Ÿè¡Œ\n",
    "try:\n",
    "    volumes_file, labels_file = prepare_real_data(CONFIG)\n",
    "    print(\"âœ… å®Ÿãƒ‡ãƒ¼ã‚¿æº–å‚™æˆåŠŸ\")\n",
    "    print(f\"   ãƒœãƒªãƒ¥ãƒ¼ãƒ : {volumes_file}\")\n",
    "    print(f\"   ãƒ©ãƒ™ãƒ«: {labels_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ å®Ÿãƒ‡ãƒ¼ã‚¿æº–å‚™ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}\")\n",
    "    print(\"ğŸ’¡ ç¢ºèªäº‹é …:\")\n",
    "    print(\"   1. kaggle.jsonãŒæ­£ã—ãé…ç½®ã•ã‚Œã¦ã„ã‚‹ã‹\")\n",
    "    print(\"   2. Kaggle APIã‚­ãƒ¼ãŒæœ‰åŠ¹ã‹\") \n",
    "    print(\"   3. Vesuvius Challengeç«¶æŠ€ã«å‚åŠ æ¸ˆã¿ã‹\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ãƒ‡ãƒ¼ã‚¿ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ä½œæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_real_data_generator(volumes_path, labels_path=None, batch_size=2, validation_split=0.2):\n",
    "    \"\"\"å®Ÿãƒ‡ãƒ¼ã‚¿ç”¨ã®TensorFlowãƒ‡ãƒ¼ã‚¿ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ä½œæˆ\"\"\"\n",
    "    print(\"ğŸ”„ å®Ÿãƒ‡ãƒ¼ã‚¿ç”¨ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ä½œæˆä¸­...\")\n",
    "    \n",
    "    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿\n",
    "    volumes = np.load(volumes_path)\n",
    "    labels = np.load(labels_path) if labels_path else None\n",
    "    \n",
    "    print(f\"ğŸ“Š ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†:\")\n",
    "    print(f\"   ãƒœãƒªãƒ¥ãƒ¼ãƒ : {volumes.shape}\")\n",
    "    print(f\"   ãƒ©ãƒ™ãƒ«: {labels.shape if labels is not None else 'None'}\")\n",
    "    \n",
    "    # è¨“ç·´/æ¤œè¨¼åˆ†å‰²\n",
    "    num_samples = len(volumes)\n",
    "    split_idx = int(num_samples * (1 - validation_split))\n",
    "    \n",
    "    train_volumes = volumes[:split_idx]\n",
    "    val_volumes = volumes[split_idx:]\n",
    "    \n",
    "    if labels is not None:\n",
    "        train_labels = labels[:split_idx]\n",
    "        val_labels = labels[split_idx:]\n",
    "    else:\n",
    "        train_labels = val_labels = None\n",
    "    \n",
    "    # TensorFlowãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ\n",
    "    def create_dataset(vols, lbls):\n",
    "        if lbls is not None:\n",
    "            dataset = tf.data.Dataset.from_tensor_slices((vols, lbls))\n",
    "        else:\n",
    "            dataset = tf.data.Dataset.from_tensor_slices(vols)\n",
    "        \n",
    "        return dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    train_dataset = create_dataset(train_volumes, train_labels)\n",
    "    val_dataset = create_dataset(val_volumes, val_labels)\n",
    "    \n",
    "    print(f\"âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆå®Œäº†:\")\n",
    "    print(f\"   è¨“ç·´: {len(train_volumes)}ã‚µãƒ³ãƒ—ãƒ«\")\n",
    "    print(f\"   æ¤œè¨¼: {len(val_volumes)}ã‚µãƒ³ãƒ—ãƒ«\")\n",
    "    \n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ä½œæˆ\n",
    "train_generator, val_generator = create_real_data_generator(\n",
    "    volumes_file, \n",
    "    labels_file, \n",
    "    batch_size=CONFIG['BATCH_SIZE']\n",
    ")\n",
    "\n",
    "print(\"âœ… ãƒ‡ãƒ¼ã‚¿ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ä½œæˆå®Œäº†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. MedicAI SwinUNETR-V2ãƒ¢ãƒ‡ãƒ«ä½œæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MedicAI ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "try:\n",
    "    from medicai.models import SwinUNETR\n",
    "    print(\"âœ… MedicAIæ—¢ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿\")\n",
    "    MEDICAI_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"ğŸ“¦ MedicAIã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­...\")\n",
    "    !pip install git+https://github.com/innat/medic-ai.git -q\n",
    "    \n",
    "    try:\n",
    "        from medicai.models import SwinUNETR\n",
    "        print(\"âœ… MedicAIã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æˆåŠŸ\")\n",
    "        MEDICAI_AVAILABLE = True\n",
    "    except ImportError as e:\n",
    "        print(f\"âŒ MedicAIã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å¤±æ•—: {e}\")\n",
    "        MEDICAI_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_swinunetr_model(config):\n",
    "    \"\"\"SwinUNETR-V2ãƒ¢ãƒ‡ãƒ«ä½œæˆ\"\"\"\n",
    "    print(\"\\nğŸ—ï¸ SwinUNETR-V2ãƒ¢ãƒ‡ãƒ«ä½œæˆä¸­...\")\n",
    "    \n",
    "    if not MEDICAI_AVAILABLE:\n",
    "        raise Exception(\"âŒ MedicAI SwinUNETRãŒåˆ©ç”¨ã§ãã¾ã›ã‚“\")\n",
    "    \n",
    "    model = SwinUNETR(\n",
    "        input_shape=config['INPUT_SHAPE'],\n",
    "        num_classes=config['NUM_CLASSES'],\n",
    "        depths=config['DEPTHS'],\n",
    "        num_heads=config['NUM_HEADS'],\n",
    "        embed_dim=config['EMBED_DIM'],\n",
    "        window_size=config['WINDOW_SIZE'],\n",
    "        spatial_dims=3,\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… MedicAI SwinUNETR-V2ä½œæˆæˆåŠŸ!\")\n",
    "    return model\n",
    "\n",
    "# SwinUNETR-V2ãƒ¢ãƒ‡ãƒ«ä½œæˆ\n",
    "model = create_swinunetr_model(CONFIG)\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«æƒ…å ±è¡¨ç¤º\n",
    "try:\n",
    "    total_params = model.count_params()\n",
    "    print(f\"\\nğŸ“Š ãƒ¢ãƒ‡ãƒ«æƒ…å ±:\")\n",
    "    print(f\"   ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {total_params:,}\")\n",
    "except:\n",
    "    print(\"âš ï¸ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°è¨ˆç®—ã‚¹ã‚­ãƒƒãƒ—\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. æå¤±é–¢æ•°ã¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹å®šç¾©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(tf.keras.losses.Loss):\n",
    "    \"\"\"Dice Loss for 3D segmentation\"\"\"\n",
    "    def __init__(self, smooth=1e-7, name=\"dice_loss\"):\n",
    "        super().__init__(name=name)\n",
    "        self.smooth = smooth\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        y_true_f = tf.keras.backend.flatten(y_true[..., 1])\n",
    "        y_pred_f = tf.keras.backend.flatten(y_pred[..., 1])\n",
    "        intersection = tf.keras.backend.sum(y_true_f * y_pred_f)\n",
    "        union = tf.keras.backend.sum(y_true_f) + tf.keras.backend.sum(y_pred_f)\n",
    "        dice = (2.0 * intersection + self.smooth) / (union + self.smooth)\n",
    "        return 1.0 - dice\n",
    "\n",
    "class FocalLoss(tf.keras.losses.Loss):\n",
    "    \"\"\"Focal Loss for class imbalance\"\"\"\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, name=\"focal_loss\"):\n",
    "        super().__init__(name=name)\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1 - tf.keras.backend.epsilon())\n",
    "        ce = -y_true * tf.math.log(y_pred)\n",
    "        weight = self.alpha * y_true * tf.math.pow((1 - y_pred), self.gamma)\n",
    "        fl = weight * ce\n",
    "        return tf.reduce_mean(tf.reduce_sum(fl, axis=-1))\n",
    "\n",
    "class CombinedLoss(tf.keras.losses.Loss):\n",
    "    \"\"\"Dice + Focal + CrossEntropy ã®çµ„ã¿åˆã‚ã›æå¤±\"\"\"\n",
    "    def __init__(self, dice_weight=0.5, focal_weight=0.3, ce_weight=0.2, name=\"combined_loss\"):\n",
    "        super().__init__(name=name)\n",
    "        self.dice_weight = dice_weight\n",
    "        self.focal_weight = focal_weight\n",
    "        self.ce_weight = ce_weight\n",
    "        \n",
    "        self.dice_loss = DiceLoss()\n",
    "        self.focal_loss = FocalLoss()\n",
    "        self.ce_loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        dice = self.dice_loss(y_true, y_pred)\n",
    "        focal = self.focal_loss(y_true, y_pred)\n",
    "        ce = self.ce_loss(y_true, y_pred)\n",
    "        \n",
    "        return (self.dice_weight * dice + \n",
    "                self.focal_weight * focal + \n",
    "                self.ce_weight * ce)\n",
    "\n",
    "class DiceScore(tf.keras.metrics.Metric):\n",
    "    \"\"\"Dice Scoreè©•ä¾¡æŒ‡æ¨™\"\"\"\n",
    "    def __init__(self, name=\"dice_score\", **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.dice_sum = self.add_weight(name=\"dice_sum\", initializer=\"zeros\")\n",
    "        self.count = self.add_weight(name=\"count\", initializer=\"zeros\")\n",
    "    \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true_f = tf.keras.backend.flatten(y_true[..., 1])\n",
    "        y_pred_f = tf.keras.backend.flatten(y_pred[..., 1])\n",
    "        intersection = tf.keras.backend.sum(y_true_f * y_pred_f)\n",
    "        union = tf.keras.backend.sum(y_true_f) + tf.keras.backend.sum(y_pred_f)\n",
    "        dice = (2.0 * intersection + 1e-7) / (union + 1e-7)\n",
    "        self.dice_sum.assign_add(dice)\n",
    "        self.count.assign_add(1)\n",
    "    \n",
    "    def result(self):\n",
    "        return self.dice_sum / self.count\n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.dice_sum.assign(0)\n",
    "        self.count.assign(0)\n",
    "\n",
    "print(\"âœ… æå¤±é–¢æ•°ã¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹å®šç¾©å®Œäº†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ãƒ¢ãƒ‡ãƒ«ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã¨å­¦ç¿’è¨­å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ\ncheckpoint_dir = Path('./checkpoints')\ncheckpoint_dir.mkdir(exist_ok=True)\n\n# å®šæœŸãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª\nepoch_checkpoint_dir = checkpoint_dir / 'epoch_checkpoints'\nepoch_checkpoint_dir.mkdir(exist_ok=True)\n\n# ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®šï¼ˆå¼·åŒ–ç‰ˆãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆï¼‰\ncallbacks = [\n    # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ä¿å­˜ï¼ˆDice ScoreåŸºæº–ï¼‰\n    tf.keras.callbacks.ModelCheckpoint(\n        'best_swinunetr_v2_real_only.h5',\n        monitor='val_dice_score',\n        save_best_only=True,\n        save_weights_only=False,\n        mode='max',\n        verbose=1,\n        save_freq='epoch'\n    ),\n    \n    # ãƒ™ã‚¹ãƒˆé‡ã¿ä¿å­˜ï¼ˆè»½é‡ç‰ˆï¼‰\n    tf.keras.callbacks.ModelCheckpoint(\n        'best_swinunetr_v2_weights_only.h5',\n        monitor='val_dice_score',\n        save_best_only=True,\n        save_weights_only=True,\n        mode='max',\n        verbose=1\n    ),\n    \n    # å®šæœŸã‚¨ãƒãƒƒã‚¯ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆï¼ˆ10ã‚¨ãƒãƒƒã‚¯æ¯ï¼‰\n    tf.keras.callbacks.ModelCheckpoint(\n        str(epoch_checkpoint_dir / 'epoch_{epoch:03d}_dice_{val_dice_score:.4f}.h5'),\n        monitor='val_dice_score',\n        save_best_only=False,\n        save_weights_only=False,\n        period=10,  # 10ã‚¨ãƒãƒƒã‚¯æ¯\n        verbose=1\n    ),\n    \n    # æœ€æ–°ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆï¼ˆä¸­æ–­æ™‚å¾©æ—§ç”¨ï¼‰\n    tf.keras.callbacks.ModelCheckpoint(\n        str(checkpoint_dir / 'latest_checkpoint.h5'),\n        monitor='val_loss',\n        save_best_only=False,\n        save_weights_only=False,\n        period=5,  # 5ã‚¨ãƒãƒƒã‚¯æ¯\n        verbose=0\n    ),\n    \n    # å­¦ç¿’ç‡å‰Šæ¸›\n    tf.keras.callbacks.ReduceLROnPlateau(\n        monitor='val_loss',\n        factor=0.5,\n        patience=10,\n        min_lr=1e-7,\n        verbose=1\n    ),\n    \n    # æ—©æœŸçµ‚äº†\n    tf.keras.callbacks.EarlyStopping(\n        monitor='val_dice_score',\n        patience=25,  # å°‘ã—é•·ã‚ã«è¨­å®š\n        restore_best_weights=True,\n        mode='max',\n        verbose=1\n    ),\n    \n    # å­¦ç¿’ãƒ­ã‚°CSV\n    tf.keras.callbacks.CSVLogger(\n        'swinunetr_v2_real_training_log.csv',\n        append=True\n    ),\n    \n    # TensorBoardï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰\n    tf.keras.callbacks.TensorBoard(\n        log_dir='./tensorboard_logs',\n        histogram_freq=0,\n        write_graph=True,\n        write_images=False,\n        update_freq='epoch'\n    )\n]\n\nprint(f\"âœ… å¼·åŒ–ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®šå®Œäº† - {len(callbacks)}å€‹\")\nprint(\"ğŸ“ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜å ´æ‰€:\")\nprint(f\"   â€¢ ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«: best_swinunetr_v2_real_only.h5\")\nprint(f\"   â€¢ ãƒ™ã‚¹ãƒˆé‡ã¿: best_swinunetr_v2_weights_only.h5\") \nprint(f\"   â€¢ å®šæœŸä¿å­˜: {epoch_checkpoint_dir}/\")\nprint(f\"   â€¢ æœ€æ–°CP: {checkpoint_dir}/latest_checkpoint.h5\")\nprint(f\"   â€¢ TensorBoard: ./tensorboard_logs/\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. å­¦ç¿’å®Ÿè¡Œ"
   ]
  },
  {
   "cell_type": "code",
   "source": "# å­¦ç¿’ç¶™ç¶šè¨­å®š\nRESUME_TRAINING = False  # Trueã«è¨­å®šã™ã‚‹ã¨æœ€æ–°ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰ç¶™ç¶š\n\ndef load_latest_checkpoint():\n    \"\"\"æœ€æ–°ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å­¦ç¿’ã‚’å†é–‹\"\"\"\n    checkpoint_files = []\n    \n    # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ãƒã‚§ãƒƒã‚¯\n    if Path('best_swinunetr_v2_real_only.h5').exists():\n        checkpoint_files.append(('best_swinunetr_v2_real_only.h5', 'best'))\n    \n    # æœ€æ–°ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ãƒã‚§ãƒƒã‚¯\n    if Path('./checkpoints/latest_checkpoint.h5').exists():\n        checkpoint_files.append(('./checkpoints/latest_checkpoint.h5', 'latest'))\n    \n    # ã‚¨ãƒãƒƒã‚¯ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ãƒã‚§ãƒƒã‚¯\n    if epoch_checkpoint_dir.exists():\n        epoch_files = list(epoch_checkpoint_dir.glob('epoch_*.h5'))\n        if epoch_files:\n            # æœ€æ–°ã®ã‚¨ãƒãƒƒã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—\n            latest_epoch = sorted(epoch_files, key=lambda x: x.stat().st_mtime)[-1]\n            checkpoint_files.append((str(latest_epoch), 'epoch'))\n    \n    if checkpoint_files:\n        print(f\"ğŸ” ç™ºè¦‹ã•ã‚ŒãŸãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ: {len(checkpoint_files)}å€‹\")\n        for cp_file, cp_type in checkpoint_files:\n            print(f\"   {cp_type}: {cp_file}\")\n        \n        # æœ€æ–°ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠï¼ˆå¤‰æ›´æ™‚åˆ»åŸºæº–ï¼‰\n        latest_checkpoint = max(checkpoint_files, key=lambda x: Path(x[0]).stat().st_mtime)\n        \n        print(f\"\\nğŸ”„ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆèª­ã¿è¾¼ã¿: {latest_checkpoint[0]}\")\n        \n        try:\n            # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿\n            loaded_model = tf.keras.models.load_model(\n                latest_checkpoint[0],\n                custom_objects={\n                    'CombinedLoss': CombinedLoss,\n                    'DiceScore': DiceScore,\n                    'DiceLoss': DiceLoss,\n                    'FocalLoss': FocalLoss\n                }\n            )\n            \n            # ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ï¼ˆå¿µã®ãŸã‚ï¼‰\n            loaded_model.compile(\n                optimizer=tf.keras.optimizers.AdamW(\n                    learning_rate=CONFIG['LEARNING_RATE'],\n                    weight_decay=CONFIG['WEIGHT_DECAY']\n                ),\n                loss=CombinedLoss(),\n                metrics=[DiceScore(), 'categorical_accuracy']\n            )\n            \n            print(f\"âœ… ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆèª­ã¿è¾¼ã¿æˆåŠŸ!\")\n            return loaded_model, True\n            \n        except Exception as e:\n            print(f\"âŒ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆèª­ã¿è¾¼ã¿å¤±æ•—: {e}\")\n            print(\"ğŸ”„ æ–°è¦å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™\")\n            return None, False\n    else:\n        print(\"ğŸ“ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ - æ–°è¦å­¦ç¿’é–‹å§‹\")\n        return None, False\n\n# ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆå‡¦ç†\nif RESUME_TRAINING:\n    checkpoint_model, checkpoint_loaded = load_latest_checkpoint()\n    if checkpoint_loaded:\n        model = checkpoint_model\n        print(\"ğŸš€ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰ã®å­¦ç¿’ç¶™ç¶šæº–å‚™å®Œäº†\")\n    else:\n        print(\"ğŸ†• æ–°è¦ãƒ¢ãƒ‡ãƒ«ã§å­¦ç¿’é–‹å§‹\")\nelse:\n    print(\"ğŸ†• æ–°è¦å­¦ç¿’ãƒ¢ãƒ¼ãƒ‰ï¼ˆRESUME_TRAINING=Falseï¼‰\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 9.5. å­¦ç¿’ç¶™ç¶šæ©Ÿèƒ½ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰\"",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ğŸš€ SwinUNETR-V2 å®Ÿãƒ‡ãƒ¼ã‚¿å­¦ç¿’é–‹å§‹!\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"   ã‚¨ãƒãƒƒã‚¯æ•°: {CONFIG['EPOCHS']}\")\n",
    "print(f\"   ãƒãƒƒãƒã‚µã‚¤ã‚º: {CONFIG['BATCH_SIZE']}\")\n",
    "print(f\"   å­¦ç¿’ç‡: {CONFIG['LEARNING_RATE']}\")\n",
    "print(f\"   ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹: Real Kaggle Data\")\n",
    "print(f\"   é–‹å§‹æ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "try:\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        validation_data=val_generator,\n",
    "        epochs=CONFIG['EPOCHS'],\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    training_duration = end_time - start_time\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ğŸ‰ å­¦ç¿’å®Œäº†!\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"   å­¦ç¿’æ™‚é–“: {training_duration}\")\n",
    "    print(f\"   å®Œäº†æ™‚åˆ»: {end_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    TRAINING_SUCCESS = True\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nâ¹ï¸ å­¦ç¿’ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ\")\n",
    "    TRAINING_SUCCESS = False\n",
    "    history = None\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ å­¦ç¿’ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    TRAINING_SUCCESS = False\n",
    "    history = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "    print(f\"\\nğŸ“ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«:\")\n    print(f\"   â€¢ best_swinunetr_v2_real_only.h5 (ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«)\")\n    print(f\"   â€¢ best_swinunetr_v2_weights_only.h5 (ãƒ™ã‚¹ãƒˆé‡ã¿)\")\n    print(f\"   â€¢ swinunetr_v2_real_final.h5 (æœ€çµ‚ãƒ¢ãƒ‡ãƒ«)\")\n    print(f\"   â€¢ swinunetr_v2_real_weights.h5 (æœ€çµ‚é‡ã¿)\")\n    print(f\"   â€¢ swinunetr_v2_real_training_log.csv (å­¦ç¿’ãƒ­ã‚°)\")\n    print(f\"   â€¢ checkpoints/latest_checkpoint.h5 (æœ€æ–°CP)\")\n    print(f\"   â€¢ checkpoints/epoch_checkpoints/ (å®šæœŸCP)\")\n    print(f\"   â€¢ tensorboard_logs/ (TensorBoard)\")\n    print(f\"   â€¢ swinunetr_v2_real_results.png (çµæœå¯è¦–åŒ–)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAINING_SUCCESS:\n",
    "    # æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ä¿å­˜\n",
    "    print(\"ğŸ’¾ æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ä¿å­˜ä¸­...\")\n",
    "    \n",
    "    try:\n",
    "        model.save('swinunetr_v2_real_final.h5')\n",
    "        model.save_weights('swinunetr_v2_real_weights.h5')\n",
    "        print(\"âœ… ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ ãƒ¢ãƒ‡ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "    \n",
    "    print(f\"\\nğŸ“ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«:\")\n",
    "    print(f\"   â€¢ best_swinunetr_v2_real_only.h5 (ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«)\")\n",
    "    print(f\"   â€¢ swinunetr_v2_real_final.h5 (æœ€çµ‚ãƒ¢ãƒ‡ãƒ«)\")\n",
    "    print(f\"   â€¢ swinunetr_v2_real_weights.h5 (é‡ã¿ã®ã¿)\")\n",
    "    print(f\"   â€¢ swinunetr_v2_real_training_log.csv (å­¦ç¿’ãƒ­ã‚°)\")\n",
    "    \n",
    "    # çµæœã‚µãƒãƒªãƒ¼\n",
    "    if history:\n",
    "        best_dice = max(history.history.get('val_dice_score', [0]))\n",
    "        best_acc = max(history.history.get('val_categorical_accuracy', [0]))\n",
    "        final_loss = history.history.get('val_loss', [0])[-1]\n",
    "        \n",
    "        print(f\"\\nğŸ¯ æœ€çµ‚çµæœ:\")\n",
    "        print(f\"   ãƒ™ã‚¹ãƒˆDice Score: {best_dice:.4f}\")\n",
    "        print(f\"   ãƒ™ã‚¹ãƒˆç²¾åº¦: {best_acc:.4f}\")\n",
    "        print(f\"   æœ€çµ‚æå¤±: {final_loss:.4f}\")\n",
    "        \n",
    "        if best_dice > 0.6:\n",
    "            print(f\"ğŸš€ ç›®æ¨™LB 0.552+ é”æˆå¯èƒ½æ€§é«˜!\")\n",
    "        elif best_dice > 0.5:\n",
    "            print(f\"âš¡ ç›®æ¨™LB 0.552+ é”æˆå¯èƒ½ - è¦æœ€é©åŒ–\")\n",
    "        else:\n",
    "            print(f\"ğŸ“Š ã•ã‚‰ãªã‚‹ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå¿…è¦\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ å­¦ç¿’ãŒå®Œäº†ã—ã¦ã„ã¾ã›ã‚“\")\n",
    "\n",
    "print(\"\\nğŸ¯ å­¦ç¿’å®Œäº† - å®ŸKaggleãƒ‡ãƒ¼ã‚¿ã®ã¿ä½¿ç”¨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. å­¦ç¿’çµæœã®å¯è¦–åŒ–ï¼ˆå­¦ç¿’å®Œäº†æ™‚ã®ã¿ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAINING_SUCCESS and history is not None:\n",
    "    print(\"ğŸ“Š å­¦ç¿’çµæœå¯è¦–åŒ–ä¸­...\")\n",
    "    \n",
    "    # ãƒ—ãƒ­ãƒƒãƒˆè¨­å®š\n",
    "    plt.style.use('default')\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('SwinUNETR-V2 Real Data Training Results - Vesuvius Challenge', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    epochs_range = range(1, len(history.history['loss']) + 1)\n",
    "    \n",
    "    # 1. Loss\n",
    "    axes[0, 0].plot(epochs_range, history.history['loss'], 'b-', label='Train Loss', linewidth=2)\n",
    "    axes[0, 0].plot(epochs_range, history.history['val_loss'], 'r-', label='Val Loss', linewidth=2)\n",
    "    axes[0, 0].set_title('Model Loss', fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Dice Score\n",
    "    axes[0, 1].plot(epochs_range, history.history['dice_score'], 'b-', label='Train Dice', linewidth=2)\n",
    "    axes[0, 1].plot(epochs_range, history.history['val_dice_score'], 'r-', label='Val Dice', linewidth=2)\n",
    "    axes[0, 1].set_title('Dice Score', fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Dice Score')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Accuracy\n",
    "    axes[1, 0].plot(epochs_range, history.history['categorical_accuracy'], 'b-', label='Train Acc', linewidth=2)\n",
    "    axes[1, 0].plot(epochs_range, history.history['val_categorical_accuracy'], 'r-', label='Val Acc', linewidth=2)\n",
    "    axes[1, 0].set_title('Categorical Accuracy', fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Accuracy')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. çµ±è¨ˆã‚µãƒãƒªãƒ¼\n",
    "    best_val_dice = max(history.history['val_dice_score'])\n",
    "    best_val_acc = max(history.history['val_categorical_accuracy'])\n",
    "    final_val_loss = history.history['val_loss'][-1]\n",
    "    \n",
    "    stats_text = f\"\"\"Training Statistics (Real Data):\n",
    "    \n",
    "Best Validation Metrics:\n",
    "â€¢ Dice Score: {best_val_dice:.4f}\n",
    "â€¢ Accuracy: {best_val_acc:.4f}\n",
    "â€¢ Final Loss: {final_val_loss:.4f}\n",
    "\n",
    "Training Details:\n",
    "â€¢ Epochs: {len(history.history['loss'])}\n",
    "â€¢ Batch Size: {CONFIG['BATCH_SIZE']}\n",
    "â€¢ Learning Rate: {CONFIG['LEARNING_RATE']}\n",
    "â€¢ Model: SwinUNETR-V2\n",
    "â€¢ Data: Real Kaggle Vesuvius\n",
    "\n",
    "Target: Kaggle LB 0.552+\"\"\"\n",
    "    \n",
    "    axes[1, 1].text(0.1, 0.9, stats_text, transform=axes[1, 1].transAxes, \n",
    "                    fontsize=10, verticalalignment='top', \n",
    "                    bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5))\n",
    "    axes[1, 1].set_xlim(0, 1)\n",
    "    axes[1, 1].set_ylim(0, 1)\n",
    "    axes[1, 1].axis('off')\n",
    "    axes[1, 1].set_title('Training Summary', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('swinunetr_v2_real_results.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"âœ… å­¦ç¿’çµæœä¿å­˜: swinunetr_v2_real_results.png\")\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸ å­¦ç¿’å±¥æ­´ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ - å¯è¦–åŒ–ã‚’ã‚¹ã‚­ãƒƒãƒ—\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. æ¨è«–ãƒ†ã‚¹ãƒˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "### ğŸ“ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«\n- `best_swinunetr_v2_real_only.h5` - ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ï¼ˆå®Œå…¨ä¿å­˜ï¼‰\n- `best_swinunetr_v2_weights_only.h5` - ãƒ™ã‚¹ãƒˆé‡ã¿ã®ã¿ï¼ˆè»½é‡ï¼‰\n- `swinunetr_v2_real_final.h5` - æœ€çµ‚ãƒ¢ãƒ‡ãƒ«  \n- `swinunetr_v2_real_weights.h5` - æœ€çµ‚é‡ã¿ã®ã¿\n- `swinunetr_v2_real_training_log.csv` - è©³ç´°å­¦ç¿’ãƒ­ã‚°\n- `checkpoints/latest_checkpoint.h5` - æœ€æ–°ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆï¼ˆç¶™ç¶šå­¦ç¿’ç”¨ï¼‰\n- `checkpoints/epoch_checkpoints/` - å®šæœŸãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆï¼ˆ10ã‚¨ãƒãƒƒã‚¯æ¯ï¼‰\n- `tensorboard_logs/` - TensorBoardå¯è¦–åŒ–ãƒ­ã‚°\n- `swinunetr_v2_real_results.png` - çµæœã‚°ãƒ©ãƒ•\n\n### ğŸ”„ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæ©Ÿèƒ½\n- **ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ä¿å­˜**: Validation Dice ScoreãŒæ”¹å–„æ™‚ã«è‡ªå‹•ä¿å­˜\n- **å®šæœŸä¿å­˜**: 10ã‚¨ãƒãƒƒã‚¯æ¯ã«æ€§èƒ½ä»˜ããƒ•ã‚¡ã‚¤ãƒ«åã§ä¿å­˜\n- **ç¶™ç¶šå­¦ç¿’**: `RESUME_TRAINING=True`ã§ä¸­æ–­æ™‚ç‚¹ã‹ã‚‰å†é–‹å¯èƒ½\n- **è»½é‡ä¿å­˜**: é‡ã¿ã®ã¿ä¿å­˜ã§ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ç¯€ç´„\n- **TensorBoard**: å­¦ç¿’æ›²ç·šãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ‰ å®Œäº†ã‚µãƒãƒªãƒ¼\n",
    "\n",
    "### âœ… å®Ÿè¡Œå†…å®¹\n",
    "- **å®Ÿãƒ‡ãƒ¼ã‚¿å°‚ç”¨**: Kaggle Vesuvius Challengeãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã¿ä½¿ç”¨\n",
    "- **SwinUNETR-V2**: æœ€æ–°ã®Shifted Window Transformerå®Ÿè£…\n",
    "- **3Då®Œå…¨å¯¾å¿œ**: 96Ã—96Ã—64ãƒœãƒªãƒ¥ãƒ¼ãƒ å‡¦ç†\n",
    "- **GPUæœ€é©åŒ–**: è‡ªå‹•GPUæ¤œå‡ºã¨ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–\n",
    "- **Mixed Precision**: é«˜é€Ÿå­¦ç¿’\n",
    "\n",
    "### ğŸ“ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«\n",
    "- `best_swinunetr_v2_real_only.h5` - ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«\n",
    "- `swinunetr_v2_real_final.h5` - æœ€çµ‚ãƒ¢ãƒ‡ãƒ«  \n",
    "- `swinunetr_v2_real_weights.h5` - é‡ã¿ã®ã¿\n",
    "- `swinunetr_v2_real_training_log.csv` - å­¦ç¿’ãƒ­ã‚°\n",
    "- `swinunetr_v2_real_results.png` - çµæœå¯è¦–åŒ–\n",
    "\n",
    "### ğŸ¯ ç›®æ¨™\n",
    "**Kaggle LB 0.552+** ã‚’ç›®æŒ‡ã—ãŸå®Ÿãƒ‡ãƒ¼ã‚¿å­¦ç¿’å®Œäº†ï¼\n",
    "\n",
    "### ğŸš€ Next Steps\n",
    "1. **Test Time Augmentation**\n",
    "2. **Ensemble Methods** \n",
    "3. **Post-processing**\n",
    "4. **Hyperparameter Tuning**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}